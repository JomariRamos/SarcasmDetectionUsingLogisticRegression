{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Sarcasm Detection Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import os,re,string,nltk,operator,math,tqdm\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "nltk.download('stopwords')\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "#SHOULD I USE LogisticRegressionCV???\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\r\n",
        "from sklearn.metrics import accuracy_score, plot_confusion_matrix, f1_score, precision_score, recall_score\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "#data augmentation\r\n",
        "!pip install googletrans==4.0.0-rc1\r\n",
        "from googletrans import Translator"
      ],
      "outputs": [],
      "metadata": {
        "id": "q9GpU6z0vGm4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "df = pd.read_csv('tweets.csv')\r\n",
        "df = df[['tweet', 'sarcastic']]\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet  sarcastic\n",
              "0  The only thing I got from college is a caffein...          1\n",
              "1  I love it when professors draw a big question ...          1\n",
              "2  Remember the hundred emails from companies whe...          1\n",
              "3  Today my pop-pop told me I was not “forced” to...          1\n",
              "4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "op7NjdLQvo94",
        "outputId": "d6eac867-8ab7-418e-9476-9b26533ae15e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "df[df.isnull().any(axis=1)]\r\n",
        "df.dropna(inplace=True)\r\n",
        "df.info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3467 entries, 0 to 3467\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   tweet      3467 non-null   object\n",
            " 1   sarcastic  3467 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 81.3+ KB\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSGMu3jXv_0P",
        "outputId": "0566d70e-a9ac-42ca-c7fa-f09ae5f2499c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def remove_punctuations(text:str)->str: \r\n",
        "  return  \"\".join(char for char in text if char not in string.punctuation)\r\n",
        "  \r\n",
        "\r\n",
        "def clean_tweet(text:str)->str:\r\n",
        "  '''Should remove hashtags and mentions first before removing punctuations'''\r\n",
        "  # remove mentions\r\n",
        "  text = re.sub('@[A-Za-z0-9_]+', '', text)\r\n",
        "\r\n",
        "  # remove hashtags\r\n",
        "  text = re.sub('#[A-Za-z0-9_]+','', text)\r\n",
        "\r\n",
        "  # remove links\r\n",
        "  text = re.sub(r\"www.\\S+\", \"\", text)\r\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\r\n",
        "  \r\n",
        "  # remove the punctuations in text from string.punctuation\r\n",
        "  text  = remove_punctuations(text)\r\n",
        "\r\n",
        "  # filter only alphanumeric characters except whitespaces\r\n",
        "  text = re.sub(\"[^A-Za-z0-9\\s]\",\"\", text)\r\n",
        "  \r\n",
        "  # replace consecutive whitespace with single space\r\n",
        "  text = re.sub('\\s+', ' ', text)\r\n",
        "  return text\r\n",
        "\r\n",
        "def tokenization(cleaned_text:str)->list:\r\n",
        "    cleaned_text = re.split('\\W+', cleaned_text)\r\n",
        "    cleaned_text = list(filter(None, cleaned_text)) #removes empty list element\r\n",
        "    return cleaned_text\r\n",
        "\r\n",
        "def remove_stopwords(tokenized_text:list)->list:\r\n",
        "\r\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "    stopwords.remove('no')# No and Not is important in sarcasm??\r\n",
        "    stopwords.remove('not')\r\n",
        "\r\n",
        "    stopwords = [remove_punctuations(word) for word in stopwords]\r\n",
        "    tokenized_text = [word for word in tokenized_text if word.lower() not in stopwords]\r\n",
        "    return tokenized_text\r\n",
        "\r\n",
        "''' Should I use these??\r\n",
        "  def stemming(text)\r\n",
        "\r\n",
        "  def lemmatizer(text)\r\n",
        "'''"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Should I use these??\\n  def stemming(text)\\n\\n  def lemmatizer(text)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7PhoYZcvwxkv",
        "outputId": "41ae8e8a-0ecb-41d9-8291-162377c0d07d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "df['tweet_cleaned'] = df['tweet'].apply(lambda x: clean_tweet(x))\r\n",
        "df['tweet_tokenized'] = df['tweet_cleaned'].apply(lambda x:tokenization(x))\r\n",
        "df['tweet_no_stopwords'] = df['tweet_tokenized'].apply(lambda x:remove_stopwords(x))\r\n",
        "print(df.sarcastic.value_counts())\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    2600\n",
            "1     867\n",
            "Name: sarcastic, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>tweet_cleaned</th>\n",
              "      <th>tweet_tokenized</th>\n",
              "      <th>tweet_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>[The, only, thing, I, got, from, college, is, ...</td>\n",
              "      <td>[thing, got, college, caffeine, addiction]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>[I, love, it, when, professors, draw, a, big, ...</td>\n",
              "      <td>[love, professors, draw, big, question, mark, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>[Remember, the, hundred, emails, from, compani...</td>\n",
              "      <td>[Remember, hundred, emails, companies, Covid, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my poppop told me I was not forced to go...</td>\n",
              "      <td>[Today, my, poppop, told, me, I, was, not, for...</td>\n",
              "      <td>[Today, poppop, told, not, forced, go, college...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I did too and I also reported Cancun Cruz not...</td>\n",
              "      <td>[I, did, too, and, I, also, reported, Cancun, ...</td>\n",
              "      <td>[also, reported, Cancun, Cruz, not, worrying, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>[The, population, spike, in, Chicago, in, 9, m...</td>\n",
              "      <td>[population, spike, Chicago, 9, months, ridicu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>Youd think in the second to last English class...</td>\n",
              "      <td>[Youd, think, in, the, second, to, last, Engli...</td>\n",
              "      <td>[think, second, last, English, class, year, pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>Im finally surfacing after a holiday to Scotla...</td>\n",
              "      <td>[Im, finally, surfacing, after, a, holiday, to...</td>\n",
              "      <td>[Im, finally, surfacing, holiday, Scotland, di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Couldnt be prouder today Well done to every st...</td>\n",
              "      <td>[Couldnt, be, prouder, today, Well, done, to, ...</td>\n",
              "      <td>[prouder, today, Well, done, every, student, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>[Overheard, as, my, 13, year, old, games, with...</td>\n",
              "      <td>[Overheard, 13, year, old, games, friend, smel...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3467 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  ...                                 tweet_no_stopwords\n",
              "0     The only thing I got from college is a caffein...  ...         [thing, got, college, caffeine, addiction]\n",
              "1     I love it when professors draw a big question ...  ...  [love, professors, draw, big, question, mark, ...\n",
              "2     Remember the hundred emails from companies whe...  ...  [Remember, hundred, emails, companies, Covid, ...\n",
              "3     Today my pop-pop told me I was not “forced” to...  ...  [Today, poppop, told, not, forced, go, college...\n",
              "4     @VolphanCarol @littlewhitty @mysticalmanatee I...  ...  [also, reported, Cancun, Cruz, not, worrying, ...\n",
              "...                                                 ...  ...                                                ...\n",
              "3463  The population spike in Chicago in 9 months is...  ...  [population, spike, Chicago, 9, months, ridicu...\n",
              "3464  You'd think in the second to last English clas...  ...  [think, second, last, English, class, year, pr...\n",
              "3465  I’m finally surfacing after a holiday to Scotl...  ...  [Im, finally, surfacing, holiday, Scotland, di...\n",
              "3466  Couldn't be prouder today. Well done to every ...  ...  [prouder, today, Well, done, every, student, g...\n",
              "3467  Overheard as my 13 year old games with a frien...  ...  [Overheard, 13, year, old, games, friend, smel...\n",
              "\n",
              "[3467 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "OX_OR8oc1nZy",
        "outputId": "d59cec42-7ef0-43e0-836a-f2b828405265"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "#Data Augmentation (We have imbalanced data: Sarcastic -> 867, Non-sarcastic -> 2600)\r\n",
        "\r\n",
        "def data_augmentation(sequence, aug_range=1, PROB = 1):\r\n",
        "    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\r\n",
        "                 'sw', 'vi', 'es', 'el']\r\n",
        "\r\n",
        "    augmented_tweets = []\r\n",
        "\r\n",
        "    #instantiate translator\r\n",
        "    translator = Translator()\r\n",
        "    \r\n",
        "    #store original language so we can convert back\r\n",
        "    org_lang = translator.detect(sequence).lang\r\n",
        "    \r\n",
        "    for i in range(0,aug_range):\r\n",
        "      #randomly choose language to translate sequence to  \r\n",
        "      random_lang = np.random.choice([lang for lang in languages if lang is not org_lang])\r\n",
        "      \r\n",
        "      if org_lang in languages:\r\n",
        "          #translate to new language and back to original\r\n",
        "          translated = translator.translate(sequence, dest = random_lang).text\r\n",
        "          #translate back to original language\r\n",
        "          translated_back = translator.translate(translated, dest = org_lang).text\r\n",
        "      \r\n",
        "          #apply with certain probability\r\n",
        "          if np.random.uniform(0, 1) <= PROB:\r\n",
        "              output_sequence = translated_back\r\n",
        "          else:\r\n",
        "              output_sequence = sequence\r\n",
        "          \r\n",
        "          augmented_tweets.append(output_sequence)\r\n",
        "          \r\n",
        "      #if detected language not in our list of languages, do nothing\r\n",
        "      else:\r\n",
        "          augmented_tweets.append(sequence)\r\n",
        "    \r\n",
        "    return augmented_tweets\r\n",
        "\r\n",
        "#check performance\r\n",
        "for i in range(5):\r\n",
        "    output = data_augmentation(\"The only thing I got from college is a caffeine addiction\")\r\n",
        "    print(output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The only thing I got from college is a caffeine addiction\n",
            "The only thing I got from university is a caffeine addiction.\n",
            "The only thing I received from the college is to stick to caffeine.\n",
            "The only thing I got from the college is a caffeine\n",
            "The only thing I got from college is a caffeine addiction\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda9UZrYECr2",
        "outputId": "915f3295-2bb1-4487-f249-d68a5171054e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "# Get max intent count to match other minority classes through data augmentation\r\n",
        "intent_count = df.sarcastic.value_counts().to_dict()\r\n",
        "max_intent_count = max(intent_count.items(), key=operator.itemgetter(1))[1]\r\n",
        "max_intent_count"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2600"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLADcdymGmkD",
        "outputId": "f811dce5-b800-4f6d-c34b-26624094884f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "## Loop to interate all messages\r\n",
        "\r\n",
        "newdf = pd.DataFrame()\r\n",
        "for intent, count in intent_count.items() :\r\n",
        "    count_diff = max_intent_count - count    ## Difference to fill\r\n",
        "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\r\n",
        "    if (multiplication_count) :\r\n",
        "        old_message_df = pd.DataFrame()\r\n",
        "        new_message_df = pd.DataFrame()\r\n",
        "        for message in tqdm.tqdm(df[df[\"sarcastic\"] == intent][\"tweet\"]) :\r\n",
        "            ## Extracting existing minority class batch\r\n",
        "            dummy1 = pd.DataFrame([message], columns=['tweet'])\r\n",
        "            dummy1[\"sarcastic\"] = intent\r\n",
        "            old_message_df = old_message_df.append(dummy1)\r\n",
        "            \r\n",
        "            ## Creating new augmented batch from existing minority class\r\n",
        "            new_messages = data_augmentation(message, multiplication_count)\r\n",
        "            dummy2 = pd.DataFrame(new_messages, columns=['tweet'])\r\n",
        "            dummy2[\"sarcastic\"] = intent\r\n",
        "            new_message_df = new_message_df.append(dummy2)\r\n",
        "        \r\n",
        "        ## Select random data points from augmented data\r\n",
        "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\r\n",
        "        \r\n",
        "        ## Merge existing and augmented data points\r\n",
        "        newdf = newdf.append([old_message_df,new_message_df])\r\n",
        "    else :\r\n",
        "        newdf = newdf.append(df[df[\"sarcastic\"] == intent])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 867/867 [1:03:31<00:00,  4.40s/it]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRW9Ct2aHOCb",
        "outputId": "33bff003-9755-4b11-e465-6a6c803d0a68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "newdf['tweet_cleaned'] = newdf['tweet'].apply(lambda x: clean_tweet(x))\r\n",
        "newdf['tweet_tokenized'] = newdf['tweet_cleaned'].apply(lambda x:tokenization(x))\r\n",
        "newdf['tweet_no_stopwords'] = newdf['tweet_tokenized'].apply(lambda x:remove_stopwords(x))\r\n",
        "print(newdf.sarcastic.value_counts())\r\n",
        "df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    2600\n",
            "0    2600\n",
            "Name: sarcastic, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>tweet_cleaned</th>\n",
              "      <th>tweet_tokenized</th>\n",
              "      <th>tweet_no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>[The, only, thing, I, got, from, college, is, ...</td>\n",
              "      <td>[thing, got, college, caffeine, addiction]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>[I, love, it, when, professors, draw, a, big, ...</td>\n",
              "      <td>[love, professors, draw, big, question, mark, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>[Remember, the, hundred, emails, from, compani...</td>\n",
              "      <td>[Remember, hundred, emails, companies, Covid, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my poppop told me I was not forced to go...</td>\n",
              "      <td>[Today, my, poppop, told, me, I, was, not, for...</td>\n",
              "      <td>[Today, poppop, told, not, forced, go, college...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I did too and I also reported Cancun Cruz not...</td>\n",
              "      <td>[I, did, too, and, I, also, reported, Cancun, ...</td>\n",
              "      <td>[also, reported, Cancun, Cruz, not, worrying, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>[The, population, spike, in, Chicago, in, 9, m...</td>\n",
              "      <td>[population, spike, Chicago, 9, months, ridicu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>Youd think in the second to last English class...</td>\n",
              "      <td>[Youd, think, in, the, second, to, last, Engli...</td>\n",
              "      <td>[think, second, last, English, class, year, pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>Im finally surfacing after a holiday to Scotla...</td>\n",
              "      <td>[Im, finally, surfacing, after, a, holiday, to...</td>\n",
              "      <td>[Im, finally, surfacing, holiday, Scotland, di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Couldnt be prouder today Well done to every st...</td>\n",
              "      <td>[Couldnt, be, prouder, today, Well, done, to, ...</td>\n",
              "      <td>[prouder, today, Well, done, every, student, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>[Overheard, as, my, 13, year, old, games, with...</td>\n",
              "      <td>[Overheard, 13, year, old, games, friend, smel...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3467 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  ...                                 tweet_no_stopwords\n",
              "0     The only thing I got from college is a caffein...  ...         [thing, got, college, caffeine, addiction]\n",
              "1     I love it when professors draw a big question ...  ...  [love, professors, draw, big, question, mark, ...\n",
              "2     Remember the hundred emails from companies whe...  ...  [Remember, hundred, emails, companies, Covid, ...\n",
              "3     Today my pop-pop told me I was not “forced” to...  ...  [Today, poppop, told, not, forced, go, college...\n",
              "4     @VolphanCarol @littlewhitty @mysticalmanatee I...  ...  [also, reported, Cancun, Cruz, not, worrying, ...\n",
              "...                                                 ...  ...                                                ...\n",
              "3463  The population spike in Chicago in 9 months is...  ...  [population, spike, Chicago, 9, months, ridicu...\n",
              "3464  You'd think in the second to last English clas...  ...  [think, second, last, English, class, year, pr...\n",
              "3465  I’m finally surfacing after a holiday to Scotl...  ...  [Im, finally, surfacing, holiday, Scotland, di...\n",
              "3466  Couldn't be prouder today. Well done to every ...  ...  [prouder, today, Well, done, every, student, g...\n",
              "3467  Overheard as my 13 year old games with a frien...  ...  [Overheard, 13, year, old, games, friend, smel...\n",
              "\n",
              "[3467 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "AEI0U7WLXX0W",
        "outputId": "13c825b2-3ba7-4d25-8e42-f6dcbe8e71ed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "#splitting data into train and test sets\n",
        "X = newdf['tweet']\n",
        "y = newdf['sarcastic']\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3,random_state=260)"
      ],
      "outputs": [],
      "metadata": {
        "id": "08eGKqUcG0QU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# processing training and testing data\n",
        "\n",
        "def clean_tweets(text)->str:\n",
        "    text:str = clean_tweet(text)\n",
        "    tokenized_text:list = tokenization(text)\n",
        "    no_stopwords:list = remove_stopwords(tokenized_text)\n",
        "    \n",
        "#We will return a string here because TfidVectorizer tokenizes the corpus.\n",
        "    tweet = \" \".join(word for word in no_stopwords)\n",
        "    return tweet #words in no_stopwords list separated by space\n",
        "\n",
        "train_X = [clean_tweets(x) for x in train_X]\n",
        "test_X= [clean_tweets(x) for x in test_X]"
      ],
      "outputs": [],
      "metadata": {
        "id": "J6G62uE6HSyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "#Text to vectors\n",
        "\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1, 2), min_df=5,max_df=0.9)\n",
        "\n",
        "tf_idf.fit(train_X)\n",
        "train_X_tfidf = tf_idf.transform(train_X)\n",
        "test_X_tfidf = tf_idf.transform(test_X)\n",
        "\n",
        "train_X_tfidf.shape\n",
        "print(test_y.value_counts())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    809\n",
            "1    751\n",
            "Name: sarcastic, dtype: int64\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRGzjE2S59p3",
        "outputId": "056c9192-2dee-44f1-97ce-d5bcec255e90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "#Fitting and evaluating model\n",
        "\n",
        "log_reg_classifier = LogisticRegression(penalty='l1',\n",
        "                                        C=10,\n",
        "                                        solver='liblinear',\n",
        "                                        random_state=260, \n",
        "                                        )\n",
        "\n",
        "log_reg_classifier.fit(train_X_tfidf,train_y)\n",
        "\n",
        "test_y_predicted = log_reg_classifier.predict(test_X_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(test_y_predicted,test_y)\n",
        "f1Score =f1_score(test_y,test_y_predicted,average='weighted')\n",
        "print(1 in test_y_predicted)\n",
        "print('Accuracy is :',accuracy)\n",
        "print('F1 score is :',f1Score)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Accuracy is : 0.676923076923077\n",
            "F1 score is : 0.6770346338825807\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD4S-5TUJCmB",
        "outputId": "013fff17-75bb-42f0-9fdf-4db9fe60d182"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#using gridsearchCV\n",
        "grid_values = {'penalty': ['l1','l2','elasticnet'], \n",
        "               'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2], \n",
        "               'solver':['liblinear','lbfgs','newton-cg','saga'],\n",
        "               'random_state':[260],\n",
        "               }\n",
        "\n",
        "log_reg=LogisticRegression()\n",
        "\n",
        "log_reg_cv=GridSearchCV(log_reg,\n",
        "                        param_grid=grid_values,\n",
        "                        cv=10,\n",
        "                        n_jobs=1,\n",
        "                        scoring='accuracy'\n",
        "                        )\n",
        "best_fit = log_reg_cv.fit(train_X_tfidf,train_y)\n",
        "\n",
        "print(\"best hyperparameters :\",best_fit.best_params_)\n",
        "print(\"accuracy :\",best_fit.best_score_)\n",
        "print(\"best estimator :\", best_fit.best_estimator_)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best hyperparameters : {'C': 10.0, 'penalty': 'l1', 'random_state': 260, 'solver': 'liblinear'}\n",
            "accuracy : 0.6942307692307692\n",
            "best estimator : LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
            "                   random_state=260, solver='liblinear', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRTCeWb8OocF",
        "outputId": "f420f8cb-7605-44e6-a40c-dcf21b6c51a7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "predicted = log_reg_classifier.predict(test_X_tfidf)\n",
        "print(\"Accuracy:\",accuracy_score(test_y, predicted))\n",
        "print(\"Precision:\",precision_score(test_y, predicted))\n",
        "print(\"Recall:\",recall_score(test_y, predicted))\n",
        "\n",
        "plot_confusion_matrix(log_reg_classifier, test_X_tfidf, test_y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.676923076923077\n",
            "Precision: 0.6585365853658537\n",
            "Recall: 0.6830892143808256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f8ed32983d0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "9QbY0eUpJfsp",
        "outputId": "0c7b7e72-2389-4877-8052-811e79c38434"
      }
    }
  ]
}